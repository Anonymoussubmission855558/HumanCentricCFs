{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# import sklearn\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the file\n",
    "cf_df = pd.read_csv('cfs.csv')\n",
    "\n",
    "conds_df = cf_df.iloc[:,-50:]\n",
    "\n",
    "use_rank = False # If True, rank features. If False, set weights manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns to confirm they were added\n",
    "print(list(cf_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify all columns containing 'credit_score'\n",
    "credit_score_columns = [col for col in cf_df.columns if 'credit_score' in col]\n",
    "\n",
    "# Step 2: Convert these columns to float\n",
    "cf_df[credit_score_columns] = cf_df[credit_score_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prefixes\n",
    "leaf_stop = 15\n",
    "prefixes = [[f\"prox{i+1}\", f\"round_prox{i+1}\"] for i in range(leaf_stop)]\n",
    "prefixes = [item for sublist in prefixes for item in sublist]\n",
    "prefixes.insert(0, 'orig')\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the groups of dummy variable prefixes\n",
    "cat_vars = ['employment_type', 'education_type']\n",
    "\n",
    "# Generate all unique pairs\n",
    "cf_cat_vars = [f\"{a}_{b}\" for a, b in product(prefixes, cat_vars)]\n",
    "# print(cf_cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "\n",
    "# Loop through each prefix and convert dummy variables back to categorical columns\n",
    "for var in cf_cat_vars:\n",
    "    # Select the columns that match the current prefix\n",
    "    dummy_cols = [col for col in cf_df.columns if var in col]\n",
    "    \n",
    "    # Revert the dummy variables to a single categorical column\n",
    "    cf_df[var] = cf_df[dummy_cols].idxmax(axis=1).str.replace(f'{var}_', '', regex=True).str.replace(\"round_\", \"\")\n",
    "\n",
    "    # Add these dummy columns to the drop list\n",
    "    columns_to_drop.extend(dummy_cols)\n",
    "\n",
    "columns_to_drop.append('Unnamed: 0')\n",
    "\n",
    "# Drop all dummy columns at once after the loop\n",
    "cf_df = cf_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher rank means more difficult to change\n",
    "ft_ranking = {\n",
    "    'education_type': 5,\n",
    "    'employment_type': 4,\n",
    "    'income': 3,\n",
    "    'credit_score': 1,\n",
    "    'amount_requested': 2,\n",
    "}\n",
    "\n",
    "# Alternative to ranking: set weights manually. Weights must add to 1.\n",
    "weights = {\n",
    "    'income': 0.1508,\n",
    "    'credit_score': 0.0497,\n",
    "    'amount_requested': 0.5467,\n",
    "    'employment_type': 0.2058,\n",
    "    'education_type': 0.0471,\n",
    "}\n",
    "\n",
    "if use_rank:\n",
    "    # Weights for each feature (define based on preferences)\n",
    "    print(\"Using ranking\")\n",
    "    weights = dict()\n",
    "    for ft in ft_ranking:\n",
    "        weights[ft] = ft_ranking[ft]*(1/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to hold all rows for each hypothesis\n",
    "all_rows_hypothesis_1 = []\n",
    "all_rows_hypothesis_2 = []\n",
    "\n",
    "# Helper function to check if a value is numeric\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define continuous columns to normalize and initialize scaler\n",
    "# continuous_columns = [\n",
    "#     'orig_income', 'orig_credit_score', 'orig_amount_requested',\n",
    "#     'prox1_income', 'prox1_credit_score', 'prox1_amount_requested',\n",
    "#     'prox2_income', 'prox2_credit_score', 'prox2_amount_requested',\n",
    "#     'prox3_income', 'prox3_credit_score', 'prox3_amount_requested',\n",
    "#     'prox4_income', 'prox4_credit_score', 'prox4_amount_requested',\n",
    "#     'round_prox1_income', 'round_prox1_credit_score', 'round_prox1_amount_requested',\n",
    "#     'round_prox2_income', 'round_prox2_credit_score', 'round_prox2_amount_requested',\n",
    "#     'round_prox3_income', 'round_prox3_credit_score', 'round_prox3_amount_requested',\n",
    "#     'round_prox4_income', 'round_prox4_credit_score', 'round_prox4_amount_requested'\n",
    "# ]\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Step 2: Fit and transform on all rows\n",
    "# normalized_df = cf_df.copy()  # Make a copy to store normalized values\n",
    "# normalized_df[continuous_columns] = scaler.fit_transform(cf_df[continuous_columns].astype(float))\n",
    "\n",
    "# # Step 3: Restore \"na\" values in the normalized copy\n",
    "# normalized_df[continuous_columns] = normalized_df[continuous_columns].fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define column groups for normalization\n",
    "income_columns = [\n",
    "    f\"{prefix}_income\"\n",
    "    for prefix in prefixes\n",
    "]\n",
    "\n",
    "credit_score_columns = [\n",
    "    f\"{prefix}_credit_score\"\n",
    "    for prefix in prefixes\n",
    "]\n",
    "\n",
    "amount_requested_columns = [\n",
    "    f\"{prefix}_amount_requested\"\n",
    "    for prefix in prefixes\n",
    "]\n",
    "\n",
    "# Step 2: Normalize each group separately\n",
    "normalized_df = cf_df.copy()  # Make a copy to store normalized values\n",
    "\n",
    "for group_columns in [income_columns, credit_score_columns, amount_requested_columns]:\n",
    "    # Calculate global min and max for the group\n",
    "    global_min = cf_df[group_columns].min().min()\n",
    "    global_max = cf_df[group_columns].max().max()\n",
    "\n",
    "    # Normalize the columns in the group\n",
    "    normalized_df[group_columns] = normalized_df[group_columns].apply(\n",
    "        lambda x: (x - global_min) / (global_max - global_min)\n",
    "    )\n",
    "\n",
    "# Step 3: Verify results\n",
    "normalized_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df['orig_credit_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df['prox1_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate proximities with normalized values for calculation only\n",
    "def calculate_proximities(orig_row, prox_row, normalized_row):\n",
    "    true_proximity = 0\n",
    "    weighted_proximity = 0\n",
    "    sparsity = 0\n",
    "    true_weight = 0.2  # Non-biased weighing\n",
    "\n",
    "    for feature, weight in weights.items():\n",
    "        if feature in ['income', 'credit_score', 'amount_requested']:\n",
    "            orig_value = normalized_row[f'orig_{feature}']\n",
    "            prox_value = normalized_row[f\"{prox_row['source']}_{feature}\"]\n",
    "            \n",
    "            # Skip if prox_value is \"na\"\n",
    "            if np.isnan(prox_value):\n",
    "                return np.nan, np.nan, np.nan\n",
    "            \n",
    "            # orig_value = float(orig_value)\n",
    "            # prox_value = float(prox_value)\n",
    "\n",
    "        else:\n",
    "            orig_value = orig_row[f'orig_{feature}']\n",
    "            prox_value = prox_row[feature]\n",
    "\n",
    "        # Continuous features\n",
    "        if feature in ['income', 'credit_score', 'amount_requested']:\n",
    "            diff = abs(orig_value - prox_value)\n",
    "            if round(orig_row[f'orig_{feature}'], 3) != round(prox_row[feature], 3):\n",
    "            #     true_proximity += diff * true_weight\n",
    "            #     weighted_proximity += diff * weight\n",
    "                sparsity += 1\n",
    "            true_proximity += diff * true_weight\n",
    "            weighted_proximity += diff * weight\n",
    "\n",
    "        # Categorical features\n",
    "        elif feature in ['employment_type', 'education_type']:\n",
    "            if orig_value != prox_value:\n",
    "                true_proximity += 1 * true_weight\n",
    "                weighted_proximity += 1 * weight\n",
    "                sparsity += 1\n",
    "\n",
    "    return true_proximity, weighted_proximity, sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df = pd.concat([cf_df, conds_df], axis=1)\n",
    "\n",
    "cf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row and store original values for output\n",
    "for index, row in cf_df.iterrows():\n",
    "    normalized_row = normalized_df.loc[index]\n",
    "\n",
    "    # Original data to be used for Hypothesis 1 and Hypothesis 2 outputs\n",
    "    original_data = {\n",
    "        'source': 'original',\n",
    "        'income': row['orig_income'],\n",
    "        'credit_score': row['orig_credit_score'],\n",
    "        'amount_requested': row['orig_amount_requested'],\n",
    "        'employment_type': row['orig_employment_type'],\n",
    "        'education_type': row['orig_education_type'],\n",
    "        'true_proximity': None,\n",
    "        'weighted_proximity': None,\n",
    "        'sparsity': None,\n",
    "        'conditions': None\n",
    "    }\n",
    "\n",
    "    original_data_norm = {\n",
    "        'source': 'original',\n",
    "        'income': normalized_row['orig_income'],\n",
    "        'credit_score': normalized_row['orig_credit_score'],\n",
    "        'amount_requested': normalized_row['orig_amount_requested'],\n",
    "        'employment_type': normalized_row['orig_employment_type'],\n",
    "        'education_type': normalized_row['orig_education_type'],\n",
    "        'true_proximity': None,\n",
    "        'weighted_proximity': None,\n",
    "        'sparsity': None\n",
    "    }\n",
    "    \n",
    "    # Append the original data to both hypotheses\n",
    "    all_rows_hypothesis_1.append(original_data)\n",
    "    all_rows_hypothesis_2.append(original_data)\n",
    "    # all_rows_hypothesis_1.append(original_data_norm)\n",
    "    # all_rows_hypothesis_2.append(original_data_norm)\n",
    "\n",
    "    # Hypothesis 1: Only unrounded counterfactuals (prox1, prox2, prox3, prox4)\n",
    "    for i in range(1, leaf_stop):\n",
    "        source_label = f\"prox{i}\"\n",
    "        \n",
    "        prox_data = {\n",
    "            'source': source_label,\n",
    "            'income': row[f'{source_label}_income'],\n",
    "            'credit_score': row[f'{source_label}_credit_score'],\n",
    "            'amount_requested': row[f'{source_label}_amount_requested'],\n",
    "            'employment_type': row.get(f'{source_label}_employment_type'),\n",
    "            'education_type': row.get(f'{source_label}_education_type'),\n",
    "        }\n",
    "\n",
    "        prox_data_norm = {\n",
    "            'source': source_label,\n",
    "            'income': normalized_row[f'{source_label}_income'],\n",
    "            'credit_score': normalized_row[f'{source_label}_credit_score'],\n",
    "            'amount_requested': normalized_row[f'{source_label}_amount_requested'],\n",
    "            'employment_type': normalized_row.get(f'{source_label}_employment_type'),\n",
    "            'education_type': normalized_row.get(f'{source_label}_education_type'),\n",
    "        }\n",
    "\n",
    "        # Calculate proximity metrics\n",
    "        true_proximity, weighted_proximity, sparsity = calculate_proximities(row, prox_data, normalized_row)\n",
    "        prox_data['true_proximity'] = true_proximity\n",
    "        prox_data['weighted_proximity'] = weighted_proximity\n",
    "        prox_data['sparsity'] = sparsity\n",
    "        # prox_data['conditions'] = row.get(f'{source_label}_conditions').iloc[0]\n",
    "\n",
    "        prox_data_norm['true_proximity'] = true_proximity\n",
    "        prox_data_norm['weighted_proximity'] = weighted_proximity\n",
    "        prox_data_norm['sparsity'] = sparsity\n",
    "\n",
    "        if not np.isnan(true_proximity):\n",
    "            all_rows_hypothesis_1.append(prox_data)\n",
    "            # all_rows_hypothesis_1.append(prox_data_norm)\n",
    "\n",
    "    # Loop for both unrounded and rounded rows in Hypothesis 2\n",
    "    for i in range(1, leaf_stop):\n",
    "        # Unrounded proximity\n",
    "        unrounded_label = f\"prox{i}\"\n",
    "\n",
    "        unrounded_data = {\n",
    "            'source': unrounded_label,\n",
    "            'income': row[f'{unrounded_label}_income'],\n",
    "            'credit_score': row[f'{unrounded_label}_credit_score'],\n",
    "            'amount_requested': row[f'{unrounded_label}_amount_requested'],\n",
    "            'employment_type': row.get(f'{unrounded_label}_employment_type'),\n",
    "            'education_type': row.get(f'{unrounded_label}_education_type'),\n",
    "        }\n",
    "\n",
    "        unrounded_data_norm = {\n",
    "            'source': unrounded_label,\n",
    "            'income': normalized_row[f'{unrounded_label}_income'],\n",
    "            'credit_score': normalized_row[f'{unrounded_label}_credit_score'],\n",
    "            'amount_requested': normalized_row[f'{unrounded_label}_amount_requested'],\n",
    "            'employment_type': normalized_row.get(f'{unrounded_label}_employment_type'),\n",
    "            'education_type': normalized_row.get(f'{unrounded_label}_education_type'),\n",
    "        }\n",
    "\n",
    "        # Calculate proximity for unrounded\n",
    "        true_proximity, weighted_proximity, sparsity = calculate_proximities(row, unrounded_data, normalized_row)\n",
    "        unrounded_data['true_proximity'] = true_proximity\n",
    "        unrounded_data['weighted_proximity'] = weighted_proximity\n",
    "        unrounded_data['sparsity'] = sparsity\n",
    "        # unrounded_data['conditions'] = row.get(f'{unrounded_label}_conditions').iloc[0]\n",
    "\n",
    "        unrounded_data_norm['true_proximity'] = true_proximity\n",
    "        unrounded_data_norm['weighted_proximity'] = weighted_proximity\n",
    "        unrounded_data_norm['sparsity'] = sparsity\n",
    "\n",
    "        if not np.isnan(true_proximity):\n",
    "            all_rows_hypothesis_2.append(unrounded_data)\n",
    "            # all_rows_hypothesis_2.append(unrounded_data_norm)\n",
    "\n",
    "        # Rounded proximity\n",
    "        rounded_label = f\"round_prox{i}\"\n",
    "\n",
    "        rounded_data = {\n",
    "            'source': rounded_label,\n",
    "            'income': row[f'{rounded_label}_income'],\n",
    "            'credit_score': row[f'{rounded_label}_credit_score'],\n",
    "            'amount_requested': row[f'{rounded_label}_amount_requested'],\n",
    "            'employment_type': row.get(f'{rounded_label}_employment_type', \"na\"),\n",
    "            'education_type': row.get(f'{rounded_label}_education_type', \"na\"),\n",
    "        }\n",
    "\n",
    "        rounded_data_norm = {\n",
    "            'source': rounded_label,\n",
    "            'income': normalized_row[f'{rounded_label}_income'],\n",
    "            'credit_score': normalized_row[f'{rounded_label}_credit_score'],\n",
    "            'amount_requested': normalized_row[f'{rounded_label}_amount_requested'],\n",
    "            'employment_type': normalized_row.get(f'{rounded_label}_employment_type', \"na\"),\n",
    "            'education_type': normalized_row.get(f'{rounded_label}_education_type', \"na\"),\n",
    "        }\n",
    "\n",
    "        # Calculate proximity for rounded\n",
    "        true_proximity, weighted_proximity, sparsity = calculate_proximities(row, rounded_data, normalized_row)\n",
    "        rounded_data['true_proximity'] = true_proximity\n",
    "        rounded_data['weighted_proximity'] = weighted_proximity\n",
    "        rounded_data['sparsity'] = sparsity\n",
    "        # rounded_data['conditions'] = row.get(f'{unrounded_label}_conditions').iloc[0]\n",
    "\n",
    "        rounded_data_norm['true_proximity'] = true_proximity\n",
    "        rounded_data_norm['weighted_proximity'] = weighted_proximity\n",
    "        rounded_data_norm['sparsity'] = sparsity\n",
    "\n",
    "        if not np.isnan(true_proximity):\n",
    "            all_rows_hypothesis_2.append(rounded_data)\n",
    "            # all_rows_hypothesis_2.append(rounded_data_norm)\n",
    "\n",
    "    # Add separator row for readability in both hypotheses\n",
    "    separator_row = {'source': '---', 'income': '---', 'credit_score': '---', 'amount_requested': '---', \n",
    "                     'employment_type': '---', 'education_type': '---', 'true_proximity': '---', \n",
    "                     'weighted_proximity': '---', 'sparsity': '---', 'conditions': '---'}\n",
    "    all_rows_hypothesis_1.append(separator_row)\n",
    "    all_rows_hypothesis_2.append(separator_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each hypothesis\n",
    "hypothesis_1_df = pd.DataFrame(all_rows_hypothesis_1)\n",
    "hypothesis_2_df = pd.DataFrame(all_rows_hypothesis_2)\n",
    "\n",
    "# Save to separate CSV files\n",
    "hypothesis_1_df.to_csv('cfs_formatted.csv', index=False)\n",
    "hypothesis_2_df.to_csv('round_cfs_formatted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
